<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><meta http-equiv="Content-Style-Type" content="text/css" /><meta name="generator" content="Aspose.Words for .NET 15.1.0.0" /><title></title></head><body><div><h1 style="margin:5pt 0pt"><span style="font-family:宋体; font-size:24pt; font-weight:bold">AI人工智能的</span><span style="font-family:宋体; font-size:24pt; font-weight:bold">几</span><span style="font-family:宋体; font-size:24pt; font-weight:bold">种常用算法</span><span style="font-family:宋体; font-size:24pt; font-weight:bold">&#xa0;</span></h1><p style="margin:0pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">ML的常用算法有个常识性的认识，没有代码，没有复杂的理论推导，就是图解一下，知道这些算法是什么，它们是怎么应用的，例子主要是分类问题。</span></p><p style="margin:0pt"><span style="font-family:宋体; font-size:12pt; font-weight:bold">决策树、随机森林算法、逻辑回归、SVM、朴素贝叶斯、K最近邻算法、K均值算法、</span><span style="font-family:宋体; font-size:12pt; font-weight:bold">Adaboost</span><span style="font-family:宋体; font-size:12pt; font-weight:bold">算法、神经网络、马尔可夫</span><span style="font-family:宋体; font-size:12pt">。</span></p><p style="margin:0pt"><span style="font-family:宋体; font-size:12pt">1. 决策树 根据一些 feature 进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.001.jpeg" width="585" height="234" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/64d4887781514c64afc5cc5004985bf8.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">2、随机森林 在</span><span style="font-family:宋体; font-size:12pt">源数据</span><span style="font-family:宋体; font-size:12pt">中随机选取数据，组成几个子集：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.002.jpeg" width="527" height="271" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/c795e4822d35481981ca42b0362b5a9d.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">S矩阵是源数据，有1-N条数据，A、B、C 是feature，最后一列C是类别：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.003.jpeg" width="634" height="297" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/8055177f960941808d29cd18db95c3d7.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">由S随机生成M</span><span style="font-family:宋体; font-size:12pt">个</span><span style="font-family:宋体; font-size:12pt">子矩阵：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.004.jpeg" width="615" height="385" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/c4b4fa46359e4bd7af36f64cd225a803.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">这M</span><span style="font-family:宋体; font-size:12pt">个</span><span style="font-family:宋体; font-size:12pt">子集得到 M </span><span style="font-family:宋体; font-size:12pt">个</span><span style="font-family:宋体; font-size:12pt">决策树：</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">将新数据投入到这M</span><span style="font-family:宋体; font-size:12pt">个</span><span style="font-family:宋体; font-size:12pt">树中，得到M</span><span style="font-family:宋体; font-size:12pt">个</span><span style="font-family:宋体; font-size:12pt">分类结果，计数</span><span style="font-family:宋体; font-size:12pt">看预测成哪</span><span style="font-family:宋体; font-size:12pt">一类的数目最多，就将此类别作为最后的预测结果。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.005.jpeg" width="645" height="368" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/873853b042f645258bd8604b70b86c53.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">3、逻辑回归 当预测目标是概率这样的，值域需要满足大于等于0，小于等于1的，这个时候单纯的线性模型是做不到的，因为在定义域不在某个范围之内时，值域也超出了规定区间。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.006.jpeg" width="635" height="370" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/faf78547a9274c4fa9d117feb449e43a.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">所以此时需要这样的形状的模型会比较好：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.007.jpeg" width="383" height="334" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/659c01a01c69475ebb94c56e02d0d5e0.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">那么怎么得到这样的模型呢？</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">这个模型需要满足两个条件 “大于等于0”，“小于等于1”</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">大于等于0 的模型可以选择绝对值，平方值，这里用指数函数，一定大于0；</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">小于等于1 用除法，分子是自己，分母是自身加上1，那一定是小于1的了。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.008.jpeg" width="629" height="370" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/9e910916fb1e4367a2e46fb70ae990c1.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">再做一下变形，就得到了 logistic regressions 模型：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.009.jpeg" width="629" height="374" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/8f23a084e386409883d79200c19cdb50.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">通过</span><span style="font-family:宋体; font-size:12pt">源数据</span><span style="font-family:宋体; font-size:12pt">计算可以得到相应的系数了：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.010.jpeg" width="626" height="294" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/9714902779ce4883bbcc66304ac71fee.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">最后得到 logistic 的图形：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.011.jpeg" width="614" height="394" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/3ed7adc69b954604b5800542fdba1d1e.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">4、SVM 要将两类分开，想要得到一个超平面，最优的超平面是到两类的 margin 达到最大，margin就是超平面与离它最近一点的距离，如下图，Z2&gt;Z1，所以绿色的超平面比较好。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.012.jpeg" width="634" height="450" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/8edbb57f6d6d4644adc09d21b98d6d5a.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">将这个超平面表示成一个线性方程，在线上方的一类，都大于等于1，另一类小于等于－1：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.013.jpeg" width="602" height="422" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/59bd7a5fb1ca445da90549494bbf634e.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">点到面的距离根据图中的公式计算：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.014.jpeg" width="421" height="341" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/7243dee058ba44f0a0eef43dbd5aa855.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">所以得到total margin的表达式如下，目标是最大化这个margin，就需要最小化分母，于是变成了一个优化问题：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.015.jpeg" width="266" height="185" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/2f0ea4d1e8de4675b204578f1f17255a.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">举个例子，三个点，找到最优的超平面，定义了 weight vector＝（2，3）－（1，1）：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.016.jpeg" width="531" height="421" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/7c6c252da0e6459390ae753e75d9cfba.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">得到weight vector为（a，2a），将两个点代入方程，代入（2，3）另其值＝1，代入（1，1）另其值＝-1，求解出 a 和 </span><span style="font-family:宋体; font-size:12pt">截矩</span><span style="font-family:宋体; font-size:12pt"> w0 的值，进而得到超平面的表达式。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.017.jpeg" width="631" height="455" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/4af6645be39c484ab09c19c4627ecd83.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">a求出来后，代入（a，2a）得到的就是support vector，</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">a和w0代入超平面的方程就是support vector machine。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">5、朴素贝叶斯</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">举个在 NLP 的应用：</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">给一段文字，返回情感分类，这段文字的态度是positive，还是negative：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.018.jpeg" width="631" height="271" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/bf26c9e0d45946ea92e7d0b7f557d57c.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">为了解决这个问题，可以只看其中的一些单词：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.019.jpeg" width="630" height="264" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/3b0173f05c3c4386bb0d1224cba1de77.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">这段文字，将仅由一些单词和它们的计数代表：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.020.jpeg" width="633" height="261" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/6dbaaf7f391a4e0fb8157176b8b0b115.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">原始问题是：给你一句话，它属于哪一类 ？</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">通过</span><span style="font-family:宋体; font-size:12pt">bayes</span><span style="font-family:宋体; font-size:12pt"> rules变成一个比较简单容易求得的问题：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.021.jpeg" width="632" height="264" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/165b37ffcec745ffa0230794f4e86b42.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">问题变成，这一类中这句话出现的概率是多少，当然，别忘了公式里的另外两个概率。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">例子：单词“love”在positive的情况下出现的概率是 0.1，在negative的情况下出现的概率是0.001。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.022.jpeg" width="640" height="268" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/a6871b33c6824e52860473c8981fd001.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">6、K最近临算法</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">给一个新的数据时，离它最近的 k </span><span style="font-family:宋体; font-size:12pt">个</span><span style="font-family:宋体; font-size:12pt">点中，哪个类别多，这个数据就属于哪一类。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">例子：要区分“猫”和“狗”，通过“claws”和“sound”两个feature来判断的话，圆形和三角形是已知分类的了，那么这个“star”代表的是哪一类呢？</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.023.jpeg" width="529" height="385" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/c2de599ff7aa4972a6ec163a7ed6f4ee.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">k＝3时，这三条线链接的点就是最近的三个点，那么圆形多一些，所以这个star就是属于猫。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.024.jpeg" width="497" height="395" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/d3a24ec4bfd94459b0248939b3e49e58.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">7、K均值算法</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">先要将一组数据，分为三类，粉色数值大，黄色数值小 。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">最开始先初始化，这里面选了最简单的 3，2，1 作为各类的初始值 。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">剩下的数据里，每个都与三个初始值计算距离，然后归类到离它最近的初始值所在类别。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.025.jpeg" width="637" height="319" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/dd2ffe9dd7474c34b06fc126c93a95a6.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">分好类后，计算每一类的平均值，作为新一轮的中心点：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.026.jpeg" width="636" height="357" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/0e48c8f9bfdc4c0b963d0c58bd665653.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">几轮之后，分组不再变化了，就可以停止了：</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.027.jpeg" width="635" height="346" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/52cbaeaa994d45c6840608bb8534775e.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.028.jpeg" width="547" height="483" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/0d4214017c7547339735ecb3f778016e.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">8、</span><span style="font-family:宋体; font-size:12pt">Adaboost</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">adaboost</span><span style="font-family:宋体; font-size:12pt"> 是 bosting 的方法之一。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">bosting就是把若干个分类效果并不好的分类器综合起来考虑，会得到一个效果比较好的分类器。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">下图，左右两个决策树，单个看是效果不怎么好的，但是把同样的数据投入进去，把两个结果加起来考虑，就会增加可信度。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.029.jpeg" width="643" height="313" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/2b6f99a9989a40ce8c070ca907cfd418.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">adaboost</span><span style="font-family:宋体; font-size:12pt"> 的例子，手写识别中，在画板上可以抓取到很多 features，例如始点的方向，始点和终点的距离等等。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.030.jpeg" width="385" height="350" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/1741a284c10f4be49b2c6cfdd357bf3e.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">training的时候，会得到每个feature的weight，例如2和3的开头部分很像，这个feature对分类起到的作用很小，它的权重也就会较小。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.031.jpeg" width="358" height="237" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/b35868bcc49845f18e218013f4645036.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">而这个alpha角就具有很强的识别性，这个feature的权重就会较大，最后的预测结果是综合考虑这些feature的结果。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.032.jpeg" width="496" height="175" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/0de8ddf174a1412fb1286069960c49a4.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">9、网络神经</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">Neural Networks适合一个input可能落入至少两个类别里：</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">NN由若干层神经元，和它们之间的联系组成。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">第一层是input层，最后一层是output层。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">在hidden层和output层都有自己的classifier。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.033.jpeg" width="362" height="277" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/763e4d0a78464b52abb33ce20a04d405.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">input输入到网络中，被激活，计算的分数被传递到下一层，激活后面的神经层，最后output层的节点上的分数代表属于各类的分数，下图例子得到分类结果为class 1；</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">同样的input被传输到不同的节点上，之所以会得到不同的结果是因为各自节点有不同的weights 和bias，这也就是forward propagation。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.034.jpeg" width="393" height="240" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/6ab94e5942a54696bb5959edbc7de6b9.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">10、马尔可夫</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">Markov Chains 由state和transitions组成。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">例子，根据这一句话 ‘the quick brown fox jumps over the lazy dog’，要得到</span><span style="font-family:宋体; font-size:12pt">markov</span><span style="font-family:宋体; font-size:12pt"> chains。</span></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">步骤，先给每一个单词设定成一个状态，然后计算状态间转换的概率。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.035.jpeg" width="630" height="236" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/0bd5c886ed714715921e0c8ad5e5d170.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">这是一句话计算出来的概率，当你用大量文本去做统计的时候，会得到更大的状态转移矩阵，例如the后面可以连接的单词，及相应的概率。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.036.jpeg" width="635" height="272" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/cddb5f22686d409eb786ffaae93b5845.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:7.55pt 0pt 21.6pt"><span style="font-family:宋体; font-size:12pt">生活中，键盘输入法的备选结果也是一样的原理，模型会更高级。</span></p><p style="margin:7.55pt 0pt 21.6pt; text-align:center"><img src="f14a8dbf-ba1c-4965-a591-027e87ba6ade.037.jpeg" width="277" height="220" alt="http://5b0988e595225.cdn.sohucs.com/images/20180924/9efbe42788524d189ea9fcd9539e6dc5.jpeg" style="-aw-left-pos:0pt; -aw-rel-hpos:column; -aw-rel-vpos:paragraph; -aw-top-pos:0pt; -aw-wrap-type:inline" /></p><p style="margin:0pt; orphans:0; text-align:justify; widows:0"><span style="font-family:等线; font-size:10.5pt">&#xa0;</span></p></div><div class="cnzz" style="display: none;">
	        <script src="https://s23.cnzz.com/z_stat.php?id=1277655852&web_id=1277655852" language="JavaScript"></script>
            </div>
            <div class="docpe" style="position: absolute;color: white;margin-left:-450;">
            <a target="_blank" href="http://www.docpe.com">档铺网——在线文档免费处理</a>
            </div>
            </body></html>